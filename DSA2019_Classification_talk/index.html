<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Classification</title>

		<meta name="description" content="Intro to Classification">
		<meta name="author" content="Mike Smith">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/white.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides" data-background="assets/pres_bg.png">
                
				<section data-background="assets/pres_bg.png">
                    
					<h1>Classification?</h1>
					<p>
						<small>Presented by Mike Smith</small><br/>
						<small>RA at the University of Sheffield</small>
                        <small><a href="mailto:m.t.smith@sheffield.ac.uk">m.t.smith@sheffield.ac.uk</a></small>
					</p>
				</section>


				<section data-background="assets/pres_bg.png">
					<h2>What is classification?</h2>
					<p>
						You have some training data. Each item consists of some values, and is of a <b>known class</b> (i.e. is labelled).
					</p>
					<p class="fragment">
					    You want to classify new data that you don't know the class (label) of.
					</p>
					<p class="fragment">
					    Because the data is labelled, this is an example of <b>supervised learning</b>.
					</p>					
                    <!-- <img src="assets/romans.jpg" width="50%" /> -->
				</section>
				
				
				<section data-background="assets/pres_bg.png">
					<h2>Topics</h2>
					<p><ul><li>Example: Malnutrition
					<li>Nearest neighbour vs decision boundary
					<li>Cross validation (& correlation)
					<li>Over-fitting
					<li>Validation
                    <li>Feature selection example: Sickle Cell
                    <li>Imbalanced Data
                    <li>Detour: F-score (what is it?)
                    <li>Adversarial Examples   </ul>
					</ul>
					</p>
				</section>




                <section data-background="assets/pres_bg.png">
					<h2>Acute Malnutrition Screening</h2>
					<p>
                        <img src="assets/SAM.jpg" width="50%" align="right" style="margin:50px;" />
                        Need to assess who needs treatment.</p><p>In-patient care provided to those with Severe Acute Malnutrition (SAM).</p><p>How to decide who has SAM?</p><br/>
					</p>
				</section>
				
				
				
				
				<section data-background="assets/pres_bg.png">
					<h2>Acute Malnutrition Screening</h2>
					<p>
                        Most common methods:</p><p style="align:left;"><ul><li>Mid-upper arm circumference</li><li>Weight-for-height z-score</li></ul></p><br/>
					</p>
				</section>
				
				
				
				<section data-background="assets/pres_bg.png">
					<h2>Acute Malnutrition Screening</h2>
					<p>
                        <img src="assets/cht_wfh_girls_z_2_5.png" width="100%" align="center" style="margin:0px; border-style:none; background:none;" />
				</section>			
				
			




                <section data-background="assets/pres_bg.png">
					<h2>Acute Malnutrition Screening</h2>
					<p>
                        <img src="assets/paper.png" width="50%" align="right" style="margin:0px; border-style:none; background:none; box-shadow: none;" />
                        Which is best?</p><p>Researchers looked at outcomes of children presenting with different MUACs and z-scores.</p><p>Aim: Classify, based on MUAC, whether a child will recover without intervention.</p><p align="left" style="font-size:12px;">e.g. Laillou, Arnaud, et al. "Optimal screening of children with acute malnutrition requires a change in current WHO guidelines as MUAC and WHZ identify different patient groups." PloS one 9.7 (2014): e101159.</small></p><br/>
					</p>
				</section>
				
				



               <section data-background="assets/pres_bg.png">
					<h2>MUAC (simulated) Dataset</h2>
					<p>
                        <img src="assets/classify1.png" align="center" style="margin:0px;" width="70%" /><br />
                        <b>SIMULATED</b> data from 29 children who recovered without treatment and 19 who didn't</p>
					</p>
				</section>



               <section data-background="assets/pres_bg.png">
					<h2>MUAC (simulated) Dataset</h2>
					<p>
                        <img src="assets/classify2.png" align="left" style="margin:10px;" width="45%" /></p>
                        <p class="fragment"><img src="assets/roc1.png" align="right" style="margin:10px;" width="45%" /></p>
                        <p><br clear="both"/>
                        Where should we put the threshold to decide who to treat?</p>
					</p>
				</section>

               <section data-background="assets/pres_bg.png">
					<h2>MUAC (simulated) Dataset</h2>
					<p>
                        <img src="assets/classify3.png" align="left" style="margin:10px;" width="45%" /></p>
                        <p class="fragment"><img src="assets/roc2.png" align="right" style="margin:10px;" width="45%" /></p>
                        <p><br clear="both"/>
                        Is this a better threshold?</p>
					</p>
				</section>

               <section data-background="assets/pres_bg.png">
					<h2>MUAC (simulated) Dataset</h2>
					<p>
                        <img src="assets/classify4.png" align="left" style="margin:10px;" width="45%" /></p>
                        <p class="fragment"><img src="assets/roc3.png" align="right" style="margin:10px;" width="45%" /></p>
                        <p><br clear="both"/>
                        What about in the middle?</p>
					</p>
			   </section>
				
               <section data-background="assets/pres_bg.png">
					<h2>MUAC (simulated) Dataset</h2>
                        <p align="left"><img src="assets/roc4.png" align="right" style="margin:10px;" width="45%" />We can keep plotting points on this graph, to generate a curve. This is called the "Receiver operating characteristic" (ROC) curve.</p>
					</p><p align="left" class="fragment">We sometimes use this to compare classifiers - area under the curve.</p><p class="fragment" align="left"><b>Cost Matrix</b>: What is the cost of a <b>False Positive</b> compared to the cost of a <b>False Negative?</b></p>
				</section>
						
                <section data-background="assets/pres_bg.png">
					<h2>More data?</h2>
				</section>
				
				<section data-background="assets/pres_bg.png">
					<h2>Use z-score instead?</h2>
                    <p>We might be able to do better with our screening if we use the z-score data?</p>
                    <p class="fragment"><img src="assets/zscore.png" align="center" style="margin:10px;" height="40%" /></p>
				</section>
							
               <section data-background="assets/pres_bg.png">
					<h2>more data?</h2>
                    <p>We might be able to do better with our screening if we combine the MUAC data with the z-score data</p>
                    <p class="fragment"><img src="assets/both.png" align="center" style="margin:10px;" height="40%" /></p>
                    <p class="fragment">Which class do people think the ? child belongs in?</p>
				</section>
				
				<section data-background="assets/pres_bg.png">
					<h2>Nearest Neighbour</h2>
                    <p><img src="assets/both.png" align="right" style="margin:10px;" height="40%" />We could assign the class of the child's "nearest neighbour".</p>
				</section>
				
				<section data-background="assets/pres_bg.png">
					<h2>k-Nearest Neighbour</h2>
                    <p><img src="assets/both.png" align="right" style="margin:10px;" height="40%" />We could look at the class of the three nearest neighbours and pick the most common class.</p>
				</section>
				
				
				<section data-background="assets/pres_bg.png">
					<h2>Linear Boundary</h2>
                    <p><img src="assets/linear.png" align="right" style="margin:10px;" height="40%" />We could draw a straight line to try to classify the children?</p>
				</section>	
				
				<section data-background="assets/pres_bg.png">
					<h2>Other classifiers</h2>
                    <p><img src="assets/plot_classifier_comparison_001.png" align="center" style="margin:10px;" />Many other types of classifier... (some are good for when there's more dimensions, or when inputs are correlated, or if there's more or less noise...)</p>
				</section>	
				
				<section data-background="assets/pres_bg.png">
					<h2>Leave-one-out cross-validation</h2>
                    <p><img src="assets/random.png" align="left" style="margin:10px;" />If we imagine our data is more random (like this). Could we still classify the children?</p>
				</section>	
							
				<section data-background="assets/pres_bg.png">
					<h2>Leave-one-out cross-validation</h2>
                    <p><img src="assets/random2.png" align="left" style="margin:10px;" />We could draw a complicated decision boundary (like this).</p><p class="fragment">All <b>training points</b> are classified correctly, but...</p><p class="fragment">...what about a new child?</p>
				</section>	
				
				<section data-background="assets/pres_bg.png">
					<h2>Leave-one-out cross-validation</h2>
                    <p>To test our classifier we need to divide the data up into <b>TRAINING</b> data and <b>TEST</b> data.</p><p class="fragment">We normally do this by leaving out one item at a time.</p>
				</section>					
				
				<section data-background="assets/pres_bg.png">
					<h2>Leave-one-out cross-validation</h2>
                    <p><img src="assets/leftout.png" align="left" style="margin:10px;" />Here we've left out one of the children from the training data. We can now TEST to see if the classifier got it correct. In this case it didn't.</p>
				</section>
							
				<section data-background="assets/pres_bg.png">
					<h2>Leave-one-out cross-validation</h2>
                    <p>Leave-one-out cross-validation allows us to say how well our classifier will <b>generalise</b> to unknown data.</p>
                    <p class="fragment"><img src="assets/twins.png" align="left" style="margin:10px;" />Warning note: When leaving 'one' out, you need to be careful to think about other correlated data. For example if each child had a twin, you might want to leave both twins out for the cross-validation.</p>
				</section>
										
				<section data-background="assets/pres_bg.png">
					<h2>Over-fitting</h2>
                    <p><img src="assets/random2.png" align="left" style="margin:10px;" />This decision boundary is an example of over-fitting.</p><p>Our 'model' is overly complicated and describes the noise, rather than the underlying structure.</p>
				</section>	
								
			    <section data-background="assets/pres_bg.png">
					<h2>Validation</h2>
                    <p>There might be many parameters we can adjust in our model (something you'll see shortly). For example the number of neighbours, or the curviness of our decision boundary.</p><p class="fragment">We could keep fiddling with these parameters (this could even be done automatically) until we get a good result on our training set...</p><p class="fragment">...does that seem 'honest'?</p>
				</section>

				<section data-background="assets/pres_bg.png">
					<h2>Validation</h2>
                    <p>Once you've finished adjusting the model (using the training and test sets), use a <b>validation</b> dataset, that you'd not used yet for anything else.</p>
                    <img src="assets/traintestvalidate.png" align="center" style="margin:30px; border-style:none; background:none; box-shadow: none;" />
				</section>
				
				<section data-background="assets/pres_bg.png">
					<h2>Feature Selection</h2>
                    <p>Typically, as a ML-expert, you'll be given a set of data and will need to decide which parts of the data will be useful to answer a problem...</p>
                    <p>Usually requires, looking at the data and discussion with the domain-experts.</p>
                    <p>Often depends on the quality and biases in the collected data<p>
				</section>
				<section data-background="assets/pres_bg.png">
					<h2>Sickle Cells</h2>
					<img src="assets/Sickle Cell Anemia 400X.jpg" width=50% />
                    <p>Example problem: Count the number of sickle cells<p>
                    <p>What features of the cells would be useful?</p>
                    <p>i.e. which features are "invariant".</p>
				</section>				
				
				<section>
<h2>Example Extracted Features</h2>				
 <table>
  <tr>
    <th>Circumference (pixels)</th>
    <th>Average pixel value</th>
    <th>Area (pixels${}^2$)</th>
    <th>Sickle? (manually labelled)<th>
  </tr>
  <tr>
    <td>45</td>
    <td>72</td>
    <td>201</td>
    <td>Yes</td>
  </tr>
  <tr>
    <td>62</td>
    <td>51</td>
    <td>340</td>
    <td>No</td>
  </tr>
  <tr>
    <td>30</td>
    <td>36</td>
    <td>85</td>
    <td>Yes</td>
  </tr>
  <tr>
    <td>55</td>
    <td>65</td>
    <td>126</td>
    <td>Yes</td>
  </tr>
  <tr>
    <td>50</td>
    <td>62</td>
    <td>125</td>
    <td>No</td>
  </tr>  
</table> 
<p>These could be fed into one of the classifiers discussed. <small>Also see Scale Invariant Feature Transforms (SIFTs).</small></p>
</section>

<section data-background="assets/pres_bg.png">
	<h2>Manual feature selection vs deep models...</h2>

    <p> 	<img src="assets/Sickle Cell Anemia 400X.jpg" width=20% align="right" />The example was of classifying images, something the CNN community have worked on considerably. Does the CNN learn the features itself? [is this data efficient?]
    <p>Many (most?) problems benefit still from feature engineering. Maybe one could localise the cells first and then run the DNN on each one, rather than the whole slide?
    <p>Can include expert knowledge (e.g. we might want to hide features that might differ in future tests - data shift)</p>
</section>		

<section data-background="assets/pres_bg.png">
<h2>Other issues: Imbalanced Data</h2>
<p>"I have a binary classification problem and one class is present with 60:1 ratio in my training set. I used the logistic regression and the result seems to just ignores one class."</p>
<p><small>https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/</small></p>
<p>Don't just report 'accuracy'! Also Area under the curve ROC isn't ideal if imbalanced. Use F-score? Or even better a confusion matrix.
<p>Some classifiers need balanced training data. Could resample to even up class sizes (or use additional synthetic data). Some classifiers let you penalise one type of mistake more.
<p>
</section>
<section data-background="assets/pres_bg.png">
<h2>Detour: F-score</h2>
<p>
<img src="assets/Precisionrecall.svg.png" align="right" width=30% />
The F-score gives a description of a classifier's accuracy combining both the <b>precision</b> and <b>recall</b>.<br/>
Example: 100 cells were tested.<br/> 25 were really sickle. The classifier found 20 of these.<br/> It also identified 30 of the non-sickle as sickle.<br/>
What is the Precision (detected true positives/all <b>detected</b> positives)?</br>
What is the Recall (detected true positives/all <b>real</b> positives)?</br>
<br/>
</p>
<p><small>Image from wikipedia</small></p>
</section>
<section data-background="assets/pres_bg.png">
<h2>Detour: F-score</h2>
<p>
What is the Precision (detected true positives/all <b>detected</b> positives)? <br/><b>20/50 = 0.4</b></br>
What is the Recall (detected true positives/all <b>real</b> positives)? <br/><b>20/25 = 0.8</b></br>
<br/>
</p>
<p>$\text{F-score} = (\frac{\text{recall}^{-1} + \text{precision}^{-1}}{2})^{-1}$
<p>$[(2.5 + 1.25)/2]^{-1} = 0.53$</p>
<p>Issue: weights precision and recall equally.</p>
</section>

    <section data-background="assets/pres_bg.png">
	    <h2>Adversarial Examples</h2>
	    <p>We take an image and add some carefully crafted noise.</p>
        <img src="assets/dog_szegedy14.png" />
        <p>Dog becomes ostrich</p>
        <p><small>Note: Noise image scaled by 10x</small></p>
        <p><small>Szegedy, et al. 2014 (original paper describing AEs)</small></p>
    </section>

    <section data-background="assets/pres_bg.png">
	    <h2>Uh oh...</h2>
        <img src="assets/uhoh.png" width=70% />
        <p><i>Note:</i> These are examples of 'test-time' attacks (we're altering the test image not the training data)</p>
        <p><small>Fischer, 2017</small></p>
    </section>
    
    <section data-background="assets/pres_bg.png">
	    <h2>Normal DNN training</h2>
	    <ul>
	    <li>Take an image and compute the class scores using a function which has <b>millions of parameters</b>.</li>
	    <li>For each parameter find how much it changes the output scores (the gradient of a cost fn wrt each weight).</li>
	    <li>Change weights to make correct class have higher score.</li>
	    <li>Repeat lots and lots of times.</li>
	    </ul>
	    <img src="assets/network.png" style="border:0px;">
        <p><small>based on http://karpathy.github.io/2015/03/30/breaking-convnets/</small></p>
    </section>
    
    <section data-background="assets/pres_bg.png">
	    <h2>Creating Adversarial Example</h2>
	    <ul>
	    <li>Take an image and compute the class scores.</li>
	    <li>For each <b>input</b> find how much it changes the output scores (the gradient of the score wrt each <b>input</b>).</li>
	    <li>Change <b>inputs</b> to make correct class have <b>lower</b> score.</li>
	    </ul>
	    <img src="assets/network.png" style="border:0px;">
        <p><small>based on http://karpathy.github.io/2015/03/30/breaking-convnets/</small></p>
        <p><small>The noise must be crafted, e.g. Fawzi, 2015 show the difference between random noise and an adversarial example.</small></p>
    </section>     

    <section data-background="assets/pres_bg.png">
        <h2>Methods for creating AEs</h2>        
        <h3>Jacobian Saliency Map algorithm - JSMA</h3>
        
        <p><img src="assets/papernot2016.png" align=right width=50%>Maximally perturb the feature (pixel) with greatest gradient (wrt class) [Papernot, 2016].
        <br/>
        <br/>They find they modify about 4% of pixels in MNIST to produce AEs.
	     </p>
	</section>
    <section data-background="assets/pres_bg.png">	     
        <h2>Others</h2>
        <p><b>Attack specific L-Norm</b>. Carlini, 2017: Good overview - basically the last two are just different norms.</p>   
        <p><b>Fast Gradient Sign Method</b>. Changes all pixels simultaneously, but by either $\pm\epsilon$.</p>
        <p><b>Black Box</b>. Papernot 2017, train a DNN on the classifications of a black-box classifier. AEs produced on their new emulation worked against the target (e.g. Amazon and Google's image recognition systems).</p>
        <p><b>Blackbox+Genetic Algorithm</b>. Su et al., 2017: single pixels. I'm not clear why we can't just check each pixel?</p>
    </section>    

    <section data-background="assets/pres_bg.png">
        <h2>Transferability</h2>
        <p><img src="assets/crosstechnique.png" style="border:0px;" width=60% align="right">Adversarial Sample created on row ML-technique and then they were tested on the column technique.
        </p>
        <p><small>Papernot 2016</small></p>
    </section> 
  
    <section data-background="assets/pres_bg.png">       
    <h2>Universal Adversarial Examples</h2>
    <p><small><img src="assets/universaladversarial.png" style="border:0px;" width=60%><br/>
    Moosavi-Dezfooli, 2016</small></p>
    </section>
    
    <section data-background="assets/pres_bg.png">
        <h2>Printing!</h2>
        <iframe width="560" height="315" src="https://www.youtube.com/embed/zQ_uMenoBCk?rel=0" frameborder="0" allowfullscreen></iframe>       
       <p><small>Alexey, 2016</small></p>
    </section>
    
    <section data-background="assets/pres_bg.png">
        <h2>Video Road Signs!</h2>
        <iframe width="560" height="315" src="https://www.youtube.com/embed/1mJMPqi2bSQ?rel=0" frameborder="0" allowfullscreen></iframe>
       <p><small>Evtimov, 2017 (published?)</small></p>
    </section>    
    
   
    <section data-background="assets/pres_bg.png">
        <h2>Why does it happen?</h2>
        <p>Originally people suggested 'overfitting', but it may be more to do with the linear nature of the classifier.
        </p>
        <img src="assets/goodfellowabstract.png" style="border:0px;" width="80%">
        <p><small>Goodfellow, 2014</small></p>
    </section>
       
    
    <section data-background="assets/pres_bg.png">
        <h2>Defences Proposed</h2>
        <p>Various ideas:</p>
        <ul>
        <li>Bradshaw et al. 2017 suggest adding a GP as a final layer.</li>
        <li>Training on a set of adversarial examples to detect them (e.g. Grosse et al. 2017)</li>
        <li>Feature selection (Zhang, 2016, Xu 2017). Also JPG compression (Dzuigaite, 2016).</li>
        <li>Mess with activation functions, etc etc</li>
        </ul>
      
    </section>
    

<section data-background="assets/pres_bg.png">
<h2>Summary</h2>
<ul>
<li>Classification is supervised learning.
<li>Example: Malnutrition (MUAC/z-score)
<li>Nearest neighbour vs decision boundary
<li>Cross validation (be careful with correlated training data)
<li>Over-fitting
<li>Validation
<li>Feature selection example: Sickle Cell
<li>Imbalanced Data
<li>Detour: F-score
<li>Adversarial Examples
</ul>
</section>
</div>

</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>

	// Full list of configuration options available at:
	// https://github.com/hakimel/reveal.js#configuration
	Reveal.initialize({
		controls: true,
		progress: true,
		history: true,
		center: true,

        math: {
            mathjax: 'https://cdn.mathjax.org/mathjax/latest/MathJax.js',
            config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
        },

		transition: 'slide', // none/fade/slide/convex/concave/zoom

		// Optional reveal.js plugins
		dependencies: [
			{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
			{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
			{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
			{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
			{ src: 'plugin/zoom-js/zoom.js', async: true },
			{ src: 'plugin/notes/notes.js', async: true },
            { src: 'plugin/math/math.js', async: true }
		]
	});

</script>

</body>
</html>
