{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "## Quick Detour, Nearest Neighbours\n",
    "\n",
    "Before we use Scikit Learn (a library for doing machine learning), we'll quickly look at how we might write a nearest neighbour classifier by hand.\n",
    "\n",
    "First we load some data. We have three arrays, the children's z-scores, their MUAC values and whether or not they are OK.\n",
    "\n",
    "We will use all but the last child to 'train' the nearest neighbour, and the last child to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#The nutrition data\n",
    "zscore=np.array([-1.59,-0.06,-2.11,0.57,1.35,0.03,0.11,-0.37,2.66,-1.24,-0.03,0.03,-0.53,3.06,1.97,1.01,0.51,-1.36,-1.44,1.45,2.55,0.4,1.03,1.72,1.,0.67,1.19,0.59,0.86,-2.16,0.87,-2.27,0.04,1.14,-0.78,1.76,-1.05,-0.7,1.58,0.11,-0.34,-2.89,0.37,0.77,0.61,-0.68,0.,-1.33])\n",
    "muac=np.array([84.5,86.6,87.2,88.5,91.3,92.4,92.4,92.8,93.3,94.4,95.2,97.4,101.4,101.5,106.1,109.5,110.8,110.9,113.3,113.6,113.6,114.2,114.8,116.,116.8,117.9,119.1,119.8,122.,122.7,123.7,124.5,124.8,125.7,126.3,129.5,130.3,131.,132.5,132.5,136.5,138.,140.,140.4,143.6,146.5,146.7,146.9])\n",
    "ok=np.array([False,False,False,False,False,False,False,False,True,False,False,False,False,True,True,True,True,False,False,True,True,False,True,True,True,True,True,True,True,False,True,False,True,True,False,True,True,True,True,True,True,False,True,True,True,True,True,True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest neighbour simply involves calculating the **distance** from the test point, to all the training points. We can find the distance of a point $x_1, y_1$ from $x_2, y_2$ by using pythagoras: \n",
    "$$\\mathtt{distance} = \\sqrt{ (x_1-x_2)^2 + (y_1-y_2)^2}$$\n",
    "\n",
    "We can get the test point (the last point in the lists):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_zscore = zscore[-1] #in python the -1 index means the end of the list\n",
    "test_muac = muac[-1]\n",
    "test_ok = ok[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to loop through all the training inputs, and find the distance from each to the test point: We want to find the training point that's got the smallest distance to the test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The algorithm predicts that the test point has a label: 'True'\n",
      "The test point actually has label: 'True'\n"
     ]
    }
   ],
   "source": [
    "smallest_distance = np.infty #this variable remembers the smallest distance that we've seen so far.\n",
    "result = np.NAN #this variable remembers the associated label for that training point.\n",
    "\n",
    "for child_zscore, child_muac, child_ok in zip(zscore[:-1],muac[:-1],ok[:-1]): #this means we loop through all but the last child\n",
    "    distance = np.sqrt((child_zscore - test_zscore)**2 + (child_muac - test_muac)**2) #get distance from test to muac\n",
    "    if distance<smallest_distance: #if it's the smallest distance we've seen, use it.\n",
    "        smallest_distance = distance #we update the smallest distance with this one\n",
    "        result = child_ok  #we make a note of the label of this training point\n",
    "        \n",
    "print(\"The algorithm predicts that the test point has a label: '%s'\" % result)\n",
    "print(\"The test point actually has label: '%s'\" % test_ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 0**: What is the problem with finding the distance like this? (hint, think about how much MUAC and z-scores change). How can we adjust the data to avoid this problem? Hint: Plotting them against each other might help you see the problem, which we'll do later (exercise 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be revisiting this dataset later, but we now turn to the 'digits' dataset and scikit learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Scikit Learn\n",
    "\n",
    "## The Digit Dataset\n",
    "\n",
    "For these classification examples we will be using scikit-learn, a toolkit for python that contains lots of methods for solving machine learning problems.\n",
    "\n",
    "It also contains some datasets we can try out. For this exercise we'll use the 'digit' dataset. This is a set of 1797 pictures of hand-drawn digits (0,1,2,3,4,5,6,7,8,9). The challenge is whether we can get the computer to learn what the digits look like, using a training set of images, and then test the computer on a test-set of images.\n",
    "\n",
    "Run the code below to import the digits dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt #plotting library (lets us draw graphs)\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets #the datasets from sklearn\n",
    "\n",
    "\n",
    "digits = datasets.load_digits() #load the digits into the variable 'digits'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the data we are going to be classifying we'll ask what shape the 'data' matrix is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that it has 1797 rows (which are the samples) and 64 columns (which are the 8x8 pixels in the data, and make up the 64 dimensions of the data set).\n",
    "\n",
    "We can have a look at just one sample. Here I'm using python's matrix 'slicing' notation. It means I want row 35 and all the columns from that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  3., 15.,  8.,  8.,  6.,  0.,  0.,  0.,  4., 16., 16., 16.,\n",
       "       13.,  2.,  0.,  0.,  3., 16.,  9.,  2.,  0.,  0.,  0.,  0.,  2.,\n",
       "       16., 16., 15.,  3.,  0.,  0.,  0.,  0.,  7.,  6., 12.,  9.,  0.,\n",
       "        0.,  0.,  0.,  0.,  1., 14., 10.,  0.,  0.,  0.,  0.,  5., 14.,\n",
       "       15.,  2.,  0.,  0.,  0.,  1., 15., 14.,  1.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data[35,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these numbers is one of the pixels in the image.\n",
    "\n",
    "It's unclear what digit this image is of still.\n",
    "\n",
    "We can draw the numbers as pixels in an image to see what image this represents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f98488d5278>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f98680eee80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#code to reshape the 64 numbers into an 8x8 matrix and then draw it\n",
    "plt.matshow(digits.data[35,:].reshape(8,8),cmap='gray') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a five!\n",
    "\n",
    "**Exercise 1:** Copy the code above into the box below, and modify it to find out what digit image 72 has inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "#Exercise 1: Your code here!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is a **supervised** learning problem, which means we need to provide labels for our data points.\n",
    "\n",
    "The labels are also in the 'digits' object. They can be accessed using 'target':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target[35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected image 35 is of the digit '5'.\n",
    "\n",
    "**Exercise 2**: What is image 72 supposed to be? Copy and alter the code above to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Exercise 2: Your code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: How many digits are of each type in the dataset?\n",
    "\n",
    "You can plot a histogram with the following code:\n",
    "\n",
    "    plt.hist(list_of_numbers,bins=range(11))\n",
    "    \n",
    "You'll need to replace \"list_of_numbers\" with the array you want to draw the histogram of.\n",
    "\n",
    "The bins parameter tells the function where to draw the boundaries of the graph. We want them at each integer.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " #Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn\n",
    "In the following we'll be using the scikit library to do the classification. We need to go through two steps: **training** (fitting) and **testing** (prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "First we need to pick some training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = digits.data[0:-10,:] #this means all but the last 10 rows should be put in training_data\n",
    "training_target = digits.target[0:-10] #this puts all but the last 10 elements of the labels (targets) into training_target\n",
    "\n",
    "#similarly this takes the last digit and puts that in test_data and test_target\n",
    "test_data = digits.data[-10:,:]\n",
    "test_target = digits.target[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training step is quite simple. Here we fit the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors #import the library that we need\n",
    "nn = neighbors.KNeighborsClassifier(n_neighbors=1) #this is our model (with just one nearest neighbour)\n",
    "nn.fit(training_data,training_target); #fit our model to the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then predict the results using the predict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 4, 8, 1, 4, 9, 0, 8, 9, 8])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of these were correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 4, 8, 8, 4, 9, 0, 8, 9, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarkably the classifier has mostly got them correct.\n",
    "\n",
    "Below is the image that it misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f984883af60>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAC89JREFUeJzt3fGr1fUdx/HXq5tSltwLq0lkZIMh\nRDCVkEVRTjFshfOH/WBQNNlwP2yRbRC1X1b/QN39MAKxukFmlKWM2FpCWgRbTe22TG2U3EixLMK0\ngkn63g/n6+bE7X7v4X4+95z7fj7g4rn3nntf73vldb7f77nf8/04IgQgl/OmegAA9VF8ICGKDyRE\n8YGEKD6QEMUHEuqJ4tteYfs92+/bvr9w1uO2j9jeUzLnjLwrbG+3vdf2u7bvKZx3ge03bb/d5D1U\nMq/JHLD9lu0XS2c1eWO237E9antn4awh25tt77e9z/Z1BbPmNz/T6bdjttcVCYuIKX2TNCDpA0nf\nkTRT0tuSri6Yd6OkRZL2VPr5LpO0qLk9W9I/Cv98lnRxc3uGpDckfb/wz/grSU9LerHS73RM0iWV\nsp6U9LPm9kxJQ5VyByR9LOnKEt+/F7b4iyW9HxEHIuKEpGck/ahUWES8JunzUt//HHmHI2J3c/u4\npH2SLi+YFxHxZfPujOat2FlatudKulXShlIZU8X2oDobisckKSJORMTRSvHLJH0QER+W+Oa9UPzL\nJX10xvsHVbAYU8n2PEkL1dkKl8wZsD0q6YikbRFRMm9Y0n2SThXMOFtIetn2LttrC+ZcJelTSU80\nhzIbbF9UMO9MqyVtKvXNe6H4Kdi+WNLzktZFxLGSWRFxMiIWSJorabHta0rk2L5N0pGI2FXi+/8f\nN0TEIkm3SPqF7RsL5ZyvzmHhoxGxUNJXkoo+ByVJtmdKWinpuVIZvVD8Q5KuOOP9uc3Hpg3bM9Qp\n/caIeKFWbrNbul3SikIR10taaXtMnUO0pbafKpT1bxFxqPn3iKQt6hwulnBQ0sEz9pg2q/NAUNot\nknZHxCelAnqh+H+T9F3bVzWPdKsl/WGKZ5o0tq3OMeK+iHi4Qt6ltoea2xdKWi5pf4msiHggIuZG\nxDx1/t9eiYg7SmSdZvsi27NP35Z0s6Qif6GJiI8lfWR7fvOhZZL2lsg6y+0quJsvdXZlplREfGP7\nl5L+rM4zmY9HxLul8mxvkrRE0iW2D0r6bUQ8VipPna3inZLeaY67Jek3EfHHQnmXSXrS9oA6D+zP\nRkSVP7NVMkfSls7jqc6X9HREvFQw725JG5uN0gFJawpmnX4wWy7p50Vzmj8dAEikF3b1AVRG8YGE\nKD6QEMUHEqL4QEI9VfzCp19OWRZ55PVaXk8VX1LNX27V/0jyyOulvF4rPoAKipzAY3tanxU0ODg4\n4a85ceKEZs6c2VXenDlzJvw1X3zxRVdzStLx48cn/DVff/21Zs2a1VXe4cOHu/q6bg0MDEz4a06d\nOqXzzutuO3ny5Mmuvq5bEeHx7jPlp+z2oyVLllTNu/fee6vm7dixo2regw8+WDVv9uzZVfOOHq31\nEv722NUHEqL4QEIUH0iI4gMJUXwgIYoPJETxgYQoPpBQq+LXXOIKQHnjFr+5aOPv1bnk79WSbrd9\ndenBAJTTZotfdYkrAOW1KX6aJa6ALCbtRTrNhQNqv2YZQBfaFL/VElcRsV7Semn6vywX6HdtdvWn\n9RJXQEbjbvFrL3EFoLxWx/jNOm+l1noDUBln7gEJUXwgIYoPJETxgYQoPpAQxQcSovhAQhQfSIiV\ndLqwZs2aqnk33XRT1bxHHnmkat7o6GjVvOHh4ap5IyMjVfPaYIsPJETxgYQoPpAQxQcSovhAQhQf\nSIjiAwlRfCAhig8kRPGBhNosofW47SO299QYCEB5bbb4I5JWFJ4DQEXjFj8iXpP0eYVZAFTCMT6Q\nEGvnAQlNWvFZOw/oH+zqAwm1+XPeJkl/kTTf9kHbPy0/FoCS2iyaeXuNQQDUw64+kBDFBxKi+EBC\nFB9IiOIDCVF8ICGKDyRE8YGEHDH5p9VP93P1582bVzWv9tpyg4ODVfNeffXVqnmrVq2qmnf06NGq\neRHh8e7DFh9IiOIDCVF8ICGKDyRE8YGEKD6QEMUHEqL4QEIUH0iI4gMJtbnY5hW2t9vea/td2/fU\nGAxAOW2uq/+NpF9HxG7bsyXtsr0tIvYWng1AIW3WzjscEbub28cl7ZN0eenBAJQzoWN82/MkLZT0\nRolhANTRegkt2xdLel7Suog4do7Ps3Ye0CdaFd/2DHVKvzEiXjjXfVg7D+gfbZ7Vt6THJO2LiIfL\njwSgtDbH+NdLulPSUtujzdsPC88FoKA2a+e9LmncS/kA6B+cuQckRPGBhCg+kBDFBxKi+EBCFB9I\niOIDCVF8IKHWL9LBf4yNjVXN27p1a9W8u+66q2reyMhI1bzaa9n1Irb4QEIUH0iI4gMJUXwgIYoP\nJETxgYQoPpAQxQcSovhAQhQfSKjNVXYvsP2m7bebtfMeqjEYgHLanKv/T0lLI+LL5vr6r9v+U0T8\ntfBsAAppc5XdkPRl8+6M5o0FM4A+1uoY3/aA7VFJRyRtiwjWzgP6WKviR8TJiFggaa6kxbavOfs+\nttfa3ml752QPCWByTehZ/Yg4Kmm7pBXn+Nz6iLg2Iq6drOEAlNHmWf1LbQ81ty+UtFzS/tKDASin\nzbP6l0l60vaAOg8Uz0bEi2XHAlBSm2f1/y5pYYVZAFTCmXtAQhQfSIjiAwlRfCAhig8kRPGBhCg+\nkBDFBxJi7bwuDA0NVc1btWpV1bzaav8+wRYfSIniAwlRfCAhig8kRPGBhCg+kBDFBxKi+EBCFB9I\niOIDCbUufrOoxlu2udAm0OcmssW/R9K+UoMAqKftElpzJd0qaUPZcQDU0HaLPyzpPkmnCs4CoJI2\nK+ncJulIROwa536snQf0iTZb/OslrbQ9JukZSUttP3X2nVg7D+gf4xY/Ih6IiLkRMU/SakmvRMQd\nxScDUAx/xwcSmtCltyJih6QdRSYBUA1bfCAhig8kRPGBhCg+kBDFBxKi+EBCFB9IiOIDCbF2XhfW\nrVtXNW9wcLBqXm1Lliypmjc8PFw1rxexxQcSovhAQhQfSIjiAwlRfCAhig8kRPGBhCg+kBDFBxKi\n+EBCrU7ZbS6tfVzSSUnfcAltoL9N5Fz9H0TEZ8UmAVANu/pAQm2LH5Jetr3L9tqSAwEor+2u/g0R\nccj2tyVts70/Il478w7NAwIPCkAfaLXFj4hDzb9HJG2RtPgc92HtPKBPtFkt9yLbs0/flnSzpD2l\nBwNQTptd/TmSttg+ff+nI+KlolMBKGrc4kfEAUnfqzALgEr4cx6QEMUHEqL4QEIUH0iI4gMJUXwg\nIYoPJETxgYRYO68LCxYsmOoRppWxsbGpHiEdtvhAQhQfSIjiAwlRfCAhig8kRPGBhCg+kBDFBxKi\n+EBCFB9IqFXxbQ/Z3mx7v+19tq8rPRiActqeq/87SS9FxI9tz5Q0q+BMAAobt/i2ByXdKOknkhQR\nJySdKDsWgJLa7OpfJelTSU/Yfsv2hmZhjf9ie63tnbZ3TvqUACZVm+KfL2mRpEcjYqGkryTdf/ad\nWEIL6B9tin9Q0sGIeKN5f7M6DwQA+tS4xY+IjyV9ZHt+86FlkvYWnQpAUW2f1b9b0sbmGf0DktaU\nGwlAaa2KHxGjkjh2B6YJztwDEqL4QEIUH0iI4gMJUXwgIYoPJETxgYQoPpAQa+d1YevWrVXzhoaG\nqubVNjIyMtUjpMMWH0iI4gMJUXwgIYoPJETxgYQoPpAQxQcSovhAQhQfSGjc4tueb3v0jLdjttfV\nGA5AGeOeshsR70laIEm2ByQdkrSl8FwACprorv4ySR9ExIclhgFQx0SLv1rSphKDAKindfGba+qv\nlPTc//g8a+cBfWIiL8u9RdLuiPjkXJ+MiPWS1kuS7ZiE2QAUMpFd/dvFbj4wLbQqfrMs9nJJL5Qd\nB0ANbZfQ+krStwrPAqASztwDEqL4QEIUH0iI4gMJUXwgIYoPJETxgYQoPpAQxQcScsTkv57G9qeS\nunnN/iWSPpvkcXohizzyauVdGRGXjnenIsXvlu2dEXHtdMsij7xey2NXH0iI4gMJ9Vrx10/TLPLI\n66m8njrGB1BHr23xAVRA8YGEKD6QEMUHEqL4QEL/Apbhi3tY5Zw7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9848663208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(test_data[3].reshape(8,8),cmap='gray') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: What was the image supposed to be, and what did the classifier think it was?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Exercise 4: Answer here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "In the lecture I briefly mentioned leave-one-out cross-validation. The same sort of idea is called **k-fold cross-validation**. In this we split the dataset into *k* groups, and train on all but one, then test on the remaining one. Then repeat while leaving out a different group.\n",
    "\n",
    "sklearn provides the 'KFold' object to let us organise our cross-validation. In the code below we repeatedly train and test, and report the accuracy in each fold.\n",
    "\n",
    "**Exercise 5**: In the code below try different classifiers and see which one does best. Try modifying the parameters to see what effect they have (e.g. the number of neighbours). Put the cursor inside the parameter brackets (e.g. on the \"n_neighbors=1\") and press SHIFT-TAB a couple of times and a box will appear describing what the different parameters do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346 of 360 correct\n",
      "343 of 360 correct\n",
      "347 of 359 correct\n",
      "355 of 359 correct\n",
      "343 of 359 correct\n",
      " \n",
      "Total: 1734 of 1797 correct (96.49%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#classification libraries\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "#prepare k-fold cross validation\n",
    "kf = KFold(len(digits.target), n_folds=5)\n",
    "KFold(n=4, n_folds=2, shuffle=False, random_state=None)\n",
    "\n",
    "#variables to count up how many we got right\n",
    "tally_correct = 0\n",
    "tally_total = 0\n",
    "for train_index, test_index in kf:\n",
    "    #here we split the dataset up into training and test sets, these change each iteration\n",
    "    training_data = digits.data[train_index,:] \n",
    "    training_target = digits.target[train_index] \n",
    "    test_data = digits.data[test_index,:]\n",
    "    test_target = digits.target[test_index]\n",
    "    \n",
    "    #TODO: Uncomment one of these classifiers to see how it does\n",
    "    #csf = tree.DecisionTreeClassifier()\n",
    "    #csf = ensemble.RandomForestClassifier(n_estimators=50, min_samples_split=1, max_depth=None, max_features=16)\n",
    "    #csf = ensemble.ExtraTreesClassifier(n_estimators=100, min_samples_split=1, max_depth=None, max_features=8)\n",
    "    csf = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "    #csf= svm.LinearSVC(C=0.05) #Linear Support Vector Machine classifier\n",
    "    #csf = naive_bayes.GaussianNB()\n",
    "    \n",
    "    csf.fit(training_data,training_target)\n",
    "    \n",
    "    predictions = csf.predict(test_data)\n",
    "    number_correct = np.sum(predictions==test_target)\n",
    "    total_number = len(predictions)\n",
    "    print(\"%d of %d correct\" % (number_correct,total_number))\n",
    "    tally_correct += number_correct\n",
    "    tally_total += total_number\n",
    "print(\" \")\n",
    "print(\"Total: %d of %d correct (%0.2f%%)\" % (tally_correct, tally_total, 100.0*tally_correct/tally_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nearest neighbour classifier did particularly well on the digits dataset. \n",
    "\n",
    "### Breast Cancer Dataset\n",
    "\n",
    "This is the wisconsin Breast Cancer dataset. It contains measurements of different cells, some of which are cancerous and some of which are not. It's been organised in the same way as before, with bc.data containing a matrix, each row is a cell, each column a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.029e+01, 1.434e+01, 1.351e+02, 1.297e+03, 1.003e-01, 1.328e-01,\n",
       "       1.980e-01, 1.043e-01, 1.809e-01, 5.883e-02, 7.572e-01, 7.813e-01,\n",
       "       5.438e+00, 9.444e+01, 1.149e-02, 2.461e-02, 5.688e-02, 1.885e-02,\n",
       "       1.756e-02, 5.115e-03, 2.254e+01, 1.667e+01, 1.522e+02, 1.575e+03,\n",
       "       1.374e-01, 2.050e-01, 4.000e-01, 1.625e-01, 2.364e-01, 7.678e-02])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc = datasets.load_breast_cancer()\n",
    "\n",
    "bc.data[4,:] #data from row number four."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find out more by running this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(bc['DESCR']) #uncomment and run to print a description of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:** Quickly try out different classifiers for the breast cancer dataset. Do the same ones do as well? If not, why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 of 114 correct\n",
      "102 of 114 correct\n",
      "109 of 114 correct\n",
      "104 of 114 correct\n",
      "104 of 113 correct\n",
      " \n",
      "Total: 516 of 569 correct (90.69%)\n"
     ]
    }
   ],
   "source": [
    "bc = datasets.load_breast_cancer()\n",
    "\n",
    "import numpy as np\n",
    "#classification libraries\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "#prepare k-fold cross validation\n",
    "kf = KFold(len(bc.target), n_folds=5)\n",
    "KFold(n=4, n_folds=2, shuffle=False, random_state=None)\n",
    "\n",
    "#variables to count up how many we got right\n",
    "tally_correct = 0\n",
    "tally_total = 0\n",
    "for train_index, test_index in kf:\n",
    "    #here we split the dataset up into training and test sets, these change each iteration\n",
    "    training_data = bc.data[train_index,:] \n",
    "    training_target = bc.target[train_index] \n",
    "    test_data = bc.data[test_index,:]\n",
    "    test_target = bc.target[test_index]\n",
    "    \n",
    "    #TODO: Uncomment one of these classifiers to see how it does\n",
    "    #csf = tree.DecisionTreeClassifier()\n",
    "    #csf = ensemble.RandomForestClassifier(n_estimators=10, min_samples_split=1, max_depth=None, max_features=5)\n",
    "    #csf = ensemble.ExtraTreesClassifier(n_estimators=100, min_samples_split=1, max_depth=None, max_features=2)\n",
    "    csf = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "    #csf= svm.LinearSVC(C=1)\n",
    "    #csf = naive_bayes.GaussianNB()\n",
    "    \n",
    "    \n",
    "    csf.fit(training_data,training_target)\n",
    "    \n",
    "    predictions = csf.predict(test_data)\n",
    "    number_correct = np.sum(predictions==test_target)\n",
    "    total_number = len(predictions)\n",
    "    print(\"%d of %d correct\" % (number_correct,total_number))\n",
    "    tally_correct += number_correct\n",
    "    tally_total += total_number\n",
    "print(\" \")\n",
    "print(\"Total: %d of %d correct (%0.2f%%)\" % (tally_correct, tally_total, 100.0*tally_correct/tally_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Nutrition (simulated) Dataset and Munging Data\n",
    "\n",
    "We often don't have data in quite the tidy format the above examples were in.\n",
    "\n",
    "As a reminder we want to guess whether a child will need treatment or not, based on their MUAC and ZSCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore=np.array([-1.59,-0.06,-2.11,0.57,1.35,0.03,0.11,-0.37,2.66,-1.24,-0.03,0.03,-0.53,3.06,1.97,1.01,0.51,-1.36,-1.44,1.45,2.55,0.4,1.03,1.72,1.,0.67,1.19,0.59,0.86,-2.16,0.87,-2.27,0.04,1.14,-0.78,1.76,-1.05,-0.7,1.58,0.11,-0.34,-2.89,0.37,0.77,0.61,-0.68,0.,-1.33])\n",
    "muac=np.array([84.5,86.6,87.2,88.5,91.3,92.4,92.4,92.8,93.3,94.4,95.2,97.4,101.4,101.5,106.1,109.5,110.8,110.9,113.3,113.6,113.6,114.2,114.8,116.,116.8,117.9,119.1,119.8,122.,122.7,123.7,124.5,124.8,125.7,126.3,129.5,130.3,131.,132.5,132.5,136.5,138.,140.,140.4,143.6,146.5,146.7,146.9])\n",
    "ok=np.array([False,False,False,False,False,False,False,False,True,False,False,False,False,True,True,True,True,False,False,True,True,False,True,True,True,True,True,True,True,False,True,False,True,True,False,True,True,True,True,True,True,False,True,True,True,True,True,True])\n",
    "\n",
    "#data for later exercise...\n",
    "edema=np.array([True,True,True,True,True,True,True,True,False,True,False,True,True,False,False,False,False,True,True,False,False,True,False,False,False,False,False,False,False,True,False,True,False,False,True,False,True,True,False,False,True,True,False,False,False,True,False,False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7**: Plot the zscores against the MUAC values\n",
    "\n",
    "**Hint:** The command you want to use is:\n",
    "\n",
    "    plt.scatter(A,B,marker='x',c='r')\n",
    "    \n",
    "where A and B are the two lists you want to plot (so you'll need to replace them). The marker parameter is optional and lets you choose the symbol in your plot. The 'c' parameter lets you choose the colour.\n",
    "\n",
    "**Extra:** You'll notice the axes are different scales (the zscores vary less than the MUAC values). To get a better idea about this, we can ask python to plot them with equal scales. After your plot command, use this command to make them equal:\n",
    "\n",
    "    plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " #Your code here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8**: Can we plot the two classes in different colours?\n",
    "\n",
    "Hint #1: First just try plotting the 'ok' children. You can get a smaller list, that just contains the ok children by selecting just the items from the lists where ok is True. This can be done with:\n",
    "\n",
    "    zscore[ok]\n",
    "    \n",
    "Hint #2: To list the not ok children you can use the not-operator \"~\":\n",
    "\n",
    "    zscore[~ok]\n",
    "    \n",
    "Hint #3: To plot both ok and not-ok children, just call the scatter function twice (one after the other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " #Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we want to try classifying the data. First we need to get it into the matrix form that we used earlier in the notebook.\n",
    "\n",
    "We need two matrices, one called 'data' and one called 'target'.\n",
    "\n",
    "'data' should be a matrix with 48 rows (for the 48 children) and two columns (for the two types of data: zscore and muac). We can combine matrices like this using numpy's vstack command.\n",
    "\n",
    "Here I've provided the code to do that for you. Much of doing data science is really about getting your data prepared into the right format and quality. The actually machine learning itself is often quite a small part of your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.vstack([zscore,muac]).T #Here I combine the zscores and MUAC. # <<< Modify for exercise 7\n",
    "target = np.array([1 if k else 0 for k in ok])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask for the shape of the data matrix, so we can confirm we've got it in the correct shape: We have 48 children, and each one has two measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 of 10 correct\n",
      "10 of 10 correct\n",
      "8 of 10 correct\n",
      "2 of 9 correct\n",
      "9 of 9 correct\n",
      " \n",
      "Total: 33 of 48 correct (68.75%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#classification libraries\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "#prepare k-fold cross validation\n",
    "kf = KFold(len(target), n_folds=5)\n",
    "KFold(n=4, n_folds=2, shuffle=False, random_state=None)\n",
    "\n",
    "#variables to count up how many we got right\n",
    "tally_correct = 0\n",
    "tally_total = 0\n",
    "for train_index, test_index in kf:\n",
    "    #here we split the dataset up into training and test sets, these change each iteration\n",
    "    training_data = data[train_index,:] \n",
    "    training_target = target[train_index] \n",
    "    test_data = data[test_index,:]\n",
    "    test_target = target[test_index]\n",
    "    \n",
    "    #TODO: Uncomment one of these classifiers to see how it does\n",
    "    #csf = tree.DecisionTreeClassifier()\n",
    "    #csf = ensemble.RandomForestClassifier(n_estimators=10, max_depth=None)\n",
    "    #csf = ensemble.ExtraTreesClassifier(n_estimators=100, max_depth=None, max_features=2)\n",
    "    #csf = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "    csf= svm.LinearSVC(C=1)\n",
    "    #csf = naive_bayes.GaussianNB()\n",
    "    \n",
    "    \n",
    "    csf.fit(training_data,training_target)\n",
    "    \n",
    "    predictions = csf.predict(test_data)\n",
    "    number_correct = np.sum(predictions==test_target)\n",
    "    total_number = len(predictions)\n",
    "    print(\"%d of %d correct\" % (number_correct,total_number))\n",
    "    tally_correct += number_correct\n",
    "    tally_total += total_number\n",
    "print(\" \")\n",
    "print(\"Total: %d of %d correct (%0.2f%%)\" % (tally_correct, tally_total, 100.0*tally_correct/tally_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also been given data about whether the child has edema (fluid build-up). Can we make use of this additional data to improve our predictions?\n",
    "\n",
    "The variable is called 'edema':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "        True, False,  True,  True, False, False, False, False,  True,\n",
       "        True, False, False,  True, False, False, False, False, False,\n",
       "       False, False,  True, False,  True, False, False,  True, False,\n",
       "        True,  True, False, False,  True,  True, False, False, False,\n",
       "        True, False, False])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a quick idea of if it's useful we can as for the confusion matrix, this counts the number of times both are true, one is true and one is false, vis-versa and when they are both false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 18],\n",
       "       [25,  4]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(ok,edema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top row is the number of children that don't need help who have edema or not (only one child in this category has edema).\n",
    "The bottom row is the number of children who need help who have edema or not. Almost all of them had edema.\n",
    "\n",
    "Clearly this variable will be very useful in our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9**: Modify the code above to include this extra variable. There is a hint in a comment to help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " #Modify code above\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Exercise**: Look at the other datasets in sklearn, and try out other classifiers on them. What datasets do you know of or have access to which might need classification?\n",
    "\n",
    "**Double Bonus Exercise**: Modify the classifier code above so that it tries different values of the neighbours parameter automatically, and returns the best value of that parameter.\n",
    "\n",
    "**Quick Quiz**\n",
    "\n",
    "How can we avoid 'cheating' when we fit (or 'train') our model?\n",
    "\n",
    "Name two types of classifier.\n",
    "\n",
    "What is over fitting?\n",
    "\n",
    "Name some features of a dataset which make one classifier work better than another?\n",
    "\n",
    "What is the difference between supervised and unsupervised learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder**: The MUAC/zscore/edema data is *simulated*, but these decisions are real decisions. However, they are also heavily based on other clinical assessments of the child's health (including HIV status, TB status, diarrhea, general overall wellness, etc)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
